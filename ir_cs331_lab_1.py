# -*- coding: utf-8 -*-
"""IR_CS331_lab-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Nfv-bqjWQ5OYcDU9FKhg6x0sagFEQdk
"""

# import necessary libraries.

import numpy as np
import nltk # natural language toolkit
import scipy.stats as ss
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
import re # regular expression for doing matching operations
from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()

# download gutenberg corpus from nltk.

nltk.download('gutenberg')

nltk.corpus.gutenberg.fileids()
['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', ...]
Emma = nltk.corpus.gutenberg.words('austen-sense.txt')

Emma

words = list(map(str.lower, Emma))
set_of_words = set(words)
words_counts = [(w, words.count(w)) for w in set_of_words]

words_counts[:15]

ranks = ss.rankdata([c for (w, c) in words_counts])
marged_ranks = [(c[0], c[1], w) for c, w in zip(words_counts, ranks)] # merging in all 3 lists

marged_ranks[:15]

sorted_ranks = sorted(marged_ranks, key=lambda rank: rank[2]) # sorting by rank

sorted_ranks[-15:] # last 15 elements in the list

Length = len(sorted_ranks)

X = np.array([np.log(Length - r + 1) for (_, __, r) in sorted_ranks])
Y = np.array([np.log(c) for (_, c, __) in sorted_ranks])

plt.plot(X, Y, 'b.')
A = np.vstack([X, np.ones(Length)]).T
m, c = np.linalg.lstsq(A, Y, rcond = None)[0]
plt.plot(X, m*X + c, 'r')
plt.xlabel('log(rank)')
plt.ylabel('log(frequency)')
plt.show()

plt.plot([Length - r + 1 for (_, __, r) in sorted_ranks], [c for (_, c, __) in sorted_ranks])
plt.xlabel('rank')
plt.ylabel('frequency')
plt.show()

sorted_ranks2 = sorted(words_counts, key=lambda freq : freq[1])
# print(sorted_ranks2[:20])

ranks_with_counts = list(enumerate(sorted_ranks2))

Length2 = len(sorted_ranks2)
X2 = np.array([np.log(Length - r + 1) for (r, (w, c)) in ranks_with_counts])
Y2 = np.array([np.log(c) for (_, (__, c)) in ranks_with_counts])
plt.plot(X2, Y2, 'b.')
# plt.show()

A2 = np.vstack([X2, np.ones(Length2)]).T
m2, c2 = np.linalg.lstsq(A2, Y2, rcond=None)[0]
plt.xlabel('log(rank)')
plt.ylabel('log(frequency)')
plt.plot(X2, m2*X2 + c2, 'r')

# np.shape(A2), np.shape(Y2)
# plt.show()

ranks_with_counts[:15]

frequency_dictionary = {}
for w, c in words_counts:
  frequency_dictionary[w] = c

# print(frequency_dictionary)

wc = WordCloud(background_color="white")

wc.generate_from_frequencies(frequency_dictionary)

plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

# previous wordcloud is just stopwords since they're the most used words

stopwords = set(STOPWORDS)
frequency_dictionary2 = {}
for w, c in words_counts:
  if w not in stopwords:
    frequency_dictionary2[w] = c

wc.generate_from_frequencies(frequency_dictionary2)

plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()

# function to remove unwanted characters from a line using regular expression library.

#the expression will only match alphanumeric characters and blank spaces.

def remove_symbols(line):
    return re.sub('[^A-Za-z0-9\s]+', '', line).lower()

stopwords = set(STOPWORDS)
frequency_dictionary2 = {}
for w, c in words_counts:
  if w not in stopwords:
    w = remove_symbols(w)
    if w:
      # print(w, porter.stem(w))
      w = porter.stem(w)
      frequency_dictionary2[w] = frequency_dictionary2.get(w, 0) + c

wc.generate_from_frequencies(frequency_dictionary2)

plt.imshow(wc, interpolation="bilinear")
plt.axis("off")
plt.show()